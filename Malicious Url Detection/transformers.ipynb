{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35943,
     "status": "ok",
     "timestamp": 1641495535111,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "4O2kTSgRbG7D",
    "outputId": "e49860f4-d329-4d4a-98c7-27316b89600a"
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 11075,
     "status": "ok",
     "timestamp": 1641495553865,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "OqkU0abVbjJa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6593,
     "status": "ok",
     "timestamp": 1641497406774,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "lCB09o4Zbtrb",
    "outputId": "e6801a2e-fd48-40e2-a9ae-d29639a74e64"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9095cb74380344b4b346df4e87cba19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b67ebc49b7e04284a76c0c78e882fa03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2e75c1b48844cbbd1bc6905f6459db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b8c9abe153f4577b689f2f70e78e878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "257d9b9851af4e99a2dae900a2c83c78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/475M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1641497415127,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "WebS7yL9ypwC"
   },
   "outputs": [],
   "source": [
    "# encode context the generation is conditioned on\n",
    "#input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='tf')\n",
    "\n",
    "input_ids = tokenizer.encode('abc.com/', return_tensors='tf')\n",
    "#input_ids = tokenizer.encode('xyz.net/', return_tensors='tf')\n",
    "\n",
    "maxLength = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1699,
     "status": "ok",
     "timestamp": 1641497422223,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "h_jJ2hvgb5MJ",
    "outputId": "ff21163b-c621-497c-b21a-774c7f05ce45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abc.com/news/local/local-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generate text until the output length (which includes the context length) reaches 50\n",
    "greedy_output = model.generate(input_ids, maxLength)\n",
    "\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3607,
     "status": "ok",
     "timestamp": 1641497432061,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "WdmTN8t-w_1F",
    "outputId": "b3965fb1-d336-4346-de06-cea330478df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abc.com/news/local-news/\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    maxLength, \n",
    "    num_beams=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4376,
     "status": "ok",
     "timestamp": 1641497448965,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "4WpJFitZxc90",
    "outputId": "1c4877bd-5337-4395-a0dd-a7eccd18bf45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abc.com/cgi-bin/curl\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "beam_output = model.generate(\n",
    "    input_ids, \n",
    "    maxLength, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4249,
     "status": "ok",
     "timestamp": 1641497463938,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "Mz2NOVvfx6Mi",
    "outputId": "214d1a81-b8f1-43d3-c4a5-4e2dc8c3eff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: abc.com/cgi-bin/curl\n",
      "1: abc.com/cgi-bin/pw\n",
      "2: abc.com/cgi-bin/python3\n",
      "3: abc.com/cgi-bin/python2\n",
      "4: abc.com/cgi-bin/python-\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "beam_outputs = model.generate(\n",
    "    input_ids, \n",
    "    maxLength, \n",
    "    num_beams=5, \n",
    "    no_repeat_ngram_size=2, \n",
    "    num_return_sequences=5, \n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# now we have 3 output sequences\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, beam_output in enumerate(beam_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1981,
     "status": "ok",
     "timestamp": 1641497626628,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "vXH1uEgZ05FF",
    "outputId": "4cb29dcf-891f-48b5-dfca-237791143926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abc.com/roadmap/2016-form\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=10,\n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1641497163822,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "mIzQNlYK2HRo",
    "outputId": "e09b98c3-5ddd-4d27-8ad1-f7108eba3cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abc.com/user/varka-\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# use temperature to decrease the sensitivity to low probability candidates\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=10, \n",
    "    top_k=0, \n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4053,
     "status": "ok",
     "timestamp": 1641497243447,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "3nPrioD42ys3",
    "outputId": "a102f009-983d-48f7-83a7-a1e862e82074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abc.com/2016/09/10/sig-of-its-own-world\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k to 50\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=20, \n",
    "    top_k=50\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4930,
     "status": "ok",
     "timestamp": 1641497277576,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "ORcEXjUA2TCl",
    "outputId": "8755ac28-788f-4a7a-ffcd-09eaa5c0e2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "abc.com/2017/07/01/new-redmond-state-where-gay\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# deactivate top_k sampling and sample only from 92% most likely words\n",
    "sample_output = model.generate(\n",
    "    input_ids, \n",
    "    do_sample=True, \n",
    "    max_length=20, \n",
    "    top_p=0.92, \n",
    "    top_k=0\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11087,
     "status": "ok",
     "timestamp": 1641497338061,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "fdc-Jzjv3VZO",
    "outputId": "33e54ffb-4a6c-42de-eecb-e44c24544f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: abc.com/2016/09/12/sad-woman-wins-tweet\n",
      "1: abc.com/2015/03/02/new-spin-jews-larry\n",
      "2: abc.com/news/local/home/larry/michael/2009/05/\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "tf.random.set_seed(0)\n",
    "\n",
    "# set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "sample_outputs = model.generate(\n",
    "    input_ids,\n",
    "    do_sample=True, \n",
    "    max_length=20, \n",
    "    top_k=50, \n",
    "    top_p=0.95, \n",
    "    num_return_sequences=3\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "for i, sample_output in enumerate(sample_outputs):\n",
    "  print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMe/9c+ALqubr7P1pnuHTxB",
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
