{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35943,
     "status": "ok",
     "timestamp": 1641495535111,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "4O2kTSgRbG7D",
    "outputId": "e49860f4-d329-4d4a-98c7-27316b89600a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -q git+https://github.com/huggingface/transformers.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 11075,
     "status": "ok",
     "timestamp": 1641495553865,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "OqkU0abVbjJa"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFGPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6593,
     "status": "ok",
     "timestamp": 1641497406774,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "lCB09o4Zbtrb",
    "outputId": "e6801a2e-fd48-40e2-a9ae-d29639a74e64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# add the EOS token as PAD token to avoid warnings\n",
    "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 386,
     "status": "ok",
     "timestamp": 1641497415127,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "WebS7yL9ypwC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "google.com/downloads/file.php?id=834\n",
      "\n",
      "The following is\n",
      "youtube.com/watch?v=XqYXqYX-X0A\n",
      "\n",
      "github.com/bitcoin/bitcoin-wallet/\n",
      "\n",
      "https://github.com/bitcoin/\n",
      "facebook.com/events/8397777/the-great-great-great-great\n",
      "amazon.com/product/B00DQQQQ/\n",
      "\n",
      "The Best of the\n",
      "whatsapp.com/\n",
      "\n",
      "http://www.facebook.com/pages/The-\n",
      "linkedin.com/\n",
      "\n",
      "http://www.facebook.com/groups/102345\n",
      "microsoft.com/en-us/library/bb81701(v=vs.85\n"
     ]
    }
   ],
   "source": [
    "# encode context the generation is conditioned on\n",
    "# input_ids = tokenizer.encode('I enjoy walking with my cute dog', return_tensors='tf')\n",
    "\n",
    "urls = ['google.com/', 'youtube.com/', 'github.com/', 'facebook.com/', 'amazon.com/', 'whatsapp.com/',\n",
    "       'linkedin.com', 'microsoft.com']\n",
    "\n",
    "\"\"\"\n",
    "urls = [\n",
    "    'weba3e13b0e15094bdfb1.com',\n",
    "    'downloadbed04b373caed221f5.co.uk',\n",
    "    'internetc85b5187e7b1572ce4.org',\n",
    "    'linuxda81dea9231d975291.fr',\n",
    "    'servere3d8fc263b3bb7eac9.co.il',\n",
    "    'serverf187b7803eb8021406.com',\n",
    "    'winzipg4599a8f9403b88bf6.co.uk',\n",
    "    'winziphfdd7727ea03806fcb.org',\n",
    "    'webi71ae8a35ce343c91e.fr'\n",
    "    ]\n",
    "\"\"\"\n",
    "\n",
    "maxLength = 20\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    # generate text until the output length (which includes the context length) reaches 50\n",
    "    greedy_output = model.generate(input_ids, maxLength)\n",
    "\n",
    "    print(tokenizer.decode(greedy_output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3607,
     "status": "ok",
     "timestamp": 1641497432061,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "WdmTN8t-w_1F",
    "outputId": "b3965fb1-d336-4346-de06-cea330478df4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "google.com/media/catalog/product\n",
      "youtube.com/watch?v=Xq\n",
      "github.com/bitcoin/bitcoin-qt/\n",
      "facebook.com/en/photos/tnc_\n",
      "amazon.com/images/content/dam/\n",
      "whatsapp.com/wp-content/\n",
      "linkedin.com\n",
      "\n",
      "http://www.\n",
      "microsoft.com/en-us/library/\n"
     ]
    }
   ],
   "source": [
    "# activate beam search and early_stopping\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "    \n",
    "    beam_output = model.generate(\n",
    "        input_ids, \n",
    "        maxLength, \n",
    "        num_beams=5, \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4376,
     "status": "ok",
     "timestamp": 1641497448965,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "4WpJFitZxc90",
    "outputId": "1c4877bd-5337-4395-a0dd-a7eccd18bf45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "google.com/media/catalog/product\n",
      "youtube.com/watch?v=Xq\n",
      "github.com/bitcoin/wiki/Bitcoin-\n",
      "facebook.com/en/photos/tnc_\n",
      "amazon.com/images/content/dam/\n",
      "whatsapp.com/wp-content/\n",
      "linkedin.com\n",
      "\n",
      "http://www.\n",
      "microsoft.com/en-us/library/\n"
     ]
    }
   ],
   "source": [
    "# set no_repeat_ngram_size to 2\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    beam_output = model.generate(\n",
    "        input_ids, \n",
    "        maxLength, \n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(beam_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4249,
     "status": "ok",
     "timestamp": 1641497463938,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "Mz2NOVvfx6Mi",
    "outputId": "214d1a81-b8f1-43d3-c4a5-4e2dc8c3eff0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: google.com/media/catalog/product\n",
      "1: google.com/en-us/library/\n",
      "2: google.com/en-us/files/\n",
      "3: google.com/downloads/file.php\n",
      "4: google.com/media/catalog/metadata\n",
      "0: youtube.com/watch?v=Xq\n",
      "1: youtube.com/watch?v=XQ\n",
      "2: youtube.com/watch?v=8q\n",
      "3: youtube.com/watch?v=3q\n",
      "4: youtube.com/watch?v=8X\n",
      "0: github.com/bitcoin/wiki/Bitcoin-\n",
      "1: github.com/bitcoin/wiki/Bitcoin_\n",
      "2: github.com/bitcoin/commit/b9\n",
      "3: github.com/bitcoin/commit/b7\n",
      "4: github.com/bitcoin/commit/3c\n",
      "0: facebook.com/en/photos/tnc_\n",
      "1: facebook.com/en/photos/a.\n",
      "2: facebook.com/en/photos/tnc/\n",
      "3: facebook.com/en/photos/tnc-\n",
      "4: facebook.com/photos/a.13315\n",
      "0: amazon.com/images/content/dam/\n",
      "1: amazon.com/images/forsale/\n",
      "2: amazon.com/images/futures/\n",
      "3: amazon.com/images/finance/prim\n",
      "4: amazon.com/images/themes/photos\n",
      "0: whatsapp.com/wp-content/\n",
      "1: whatsapp.com/en-us/\n",
      "2: whatsapp.com/en-US/\n",
      "3: whatsapp.com/\n",
      "\n",
      "http://\n",
      "4: whatsapp.com/news/local/\n",
      "0: linkedin.com\n",
      "\n",
      "http://www.\n",
      "1: linkedin.com/\n",
      "\n",
      "http://www\n",
      "2: linkedin.com\n",
      "3: linkedin.com\n",
      "\n",
      "http://bit.\n",
      "4: linkedin.com/en/\n",
      "\n",
      "http\n",
      "0: microsoft.com/en-us/library/\n",
      "1: microsoft.com/en-us/download/\n",
      "2: microsoft.com/en-us/downloads\n",
      "3: microsoft.com/en-us/Product/\n",
      "4: microsoft.com/cgi-bin/pw\n"
     ]
    }
   ],
   "source": [
    "# set return_num_sequences > 1\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    beam_outputs = model.generate(\n",
    "        input_ids, \n",
    "        maxLength, \n",
    "        num_beams=5, \n",
    "        no_repeat_ngram_size=2, \n",
    "        num_return_sequences=5, \n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # now we have 3 output sequences\n",
    "\n",
    "    for i, beam_output in enumerate(beam_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(beam_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1981,
     "status": "ok",
     "timestamp": 1641497626628,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "vXH1uEgZ05FF",
    "outputId": "4cb29dcf-891f-48b5-dfca-237791143926"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "google.com/store/apps/details?\n",
      "youtube.com/watch?v=7N\n",
      "github.com/sequio/sequio-\n",
      "facebook.com/banner\n",
      "\n",
      "E-\n",
      "amazon.com/ Jim Packard 19/12\n",
      "whatsapp.com/2016/09/\n",
      "linkedin.com/pyconf/\n",
      "\n",
      "\n",
      "microsoft.com/product/79206036\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # activate sampling and deactivate top_k by setting top_k sampling to 0\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=10,\n",
    "        top_k=0\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1827,
     "status": "ok",
     "timestamp": 1641497163822,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "mIzQNlYK2HRo",
    "outputId": "e09b98c3-5ddd-4d27-8ad1-f7108eba3cfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "google.com/watch?v=5K\n",
      "youtube.com/watch?v=5L\n",
      "github.com/rs/bb/sig\n",
      "facebook.com/us/rodney-j\n",
      "amazon.com/item/27382526\n",
      "whatsapp.com/2015/07/\n",
      "linkedin.com/2017/08/the\n",
      "microsoft.com/...\n",
      "\n",
      "4. B\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # use temperature to decrease the sensitivity to low probability candidates\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=10, \n",
    "        top_k=0, \n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "\n",
    "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4053,
     "status": "ok",
     "timestamp": 1641497243447,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "3nPrioD42ys3",
    "outputId": "a102f009-983d-48f7-83a7-a1e862e82074"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "google.com/item/com.hud\n",
      "youtube.com/watch?v=4K\n",
      "github.com/zendesk/p\n",
      "facebook.com/en/photos/a.\n",
      "amazon.com/collections/s/S\n",
      "whatsapp.com/2014/09/\n",
      "linkedin.com/post/202220\n",
      "microsoft.com/media/catalog/product\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # set top_k to 50\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=10, \n",
    "        top_k=50\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4930,
     "status": "ok",
     "timestamp": 1641497277576,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "ORcEXjUA2TCl",
    "outputId": "8755ac28-788f-4a7a-ffcd-09eaa5c0e2b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "google.com/group/SHOS.\n",
      "\n",
      "youtube.com/watch?v=6M\n",
      "github.com/chengamp@gmail.\n",
      "facebook.com/21TheLifeSpin/\n",
      "amazon.com/2015/07/01/\n",
      "whatsapp.com/listen/39\n",
      "linkedin.com/group/head_ru\n",
      "microsoft.com/infosec/workflow\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # deactivate top_k sampling and sample only from 92% most likely words\n",
    "    sample_output = model.generate(\n",
    "        input_ids, \n",
    "        do_sample=True, \n",
    "        max_length=10, \n",
    "        top_p=0.92, \n",
    "        top_k=0\n",
    "    )\n",
    "\n",
    "    print(tokenizer.decode(sample_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11087,
     "status": "ok",
     "timestamp": 1641497338061,
     "user": {
      "displayName": "oren",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "03155878310696212033"
     },
     "user_tz": -120
    },
    "id": "fdc-Jzjv3VZO",
    "outputId": "33e54ffb-4a6c-42de-eecb-e44c24544f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "0: google.com/item/v3-4\n",
      "1: google.com/watch?v=t-\n",
      "2: google.com/downloads/details?id\n",
      "0: youtube.com/watch?v=4K\n",
      "1: youtube.com/watch?v=r-\n",
      "2: youtube.com/watch?v=gZ\n",
      "0: github.com/xenocontrol/\n",
      "1: github.com/u/gameloft/\n",
      "2: github.com/google/android/framework/\n",
      "0: facebook.com/en/photos/a.\n",
      "1: facebook.com/en-us/articles/\n",
      "2: facebook.com/news/local/local/\n",
      "0: amazon.com/collections/s/S\n",
      "1: amazon.com/collections/books/1\n",
      "2: amazon.com/media/catalog/product\n",
      "0: whatsapp.com/2014/09/\n",
      "1: whatsapp.com/2014/11/\n",
      "2: whatsapp.com/product/1806\n",
      "0: linkedin.com/2013/05/the\n",
      "1: linkedin.com/v/1478-\n",
      "2: linkedin.com/wp-content/uploads\n",
      "0: microsoft.com/product/15202214\n",
      "1: microsoft.com/en-US/x86\n",
      "2: microsoft.com/images/article/news/\n"
     ]
    }
   ],
   "source": [
    "# set seed to reproduce results. Feel free to change the seed though to get different results\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=10, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "      print(\"{}: {}\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create data set of new generated urls based on popular websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  url label\n",
      "0        support.google.com/books?id=  good\n",
      "1           support.google.com/en-GB/  good\n",
      "2     support.google.com/site/public/  good\n",
      "3              youtube.com/watch?v=7X  good\n",
      "4              youtube.com/watch?v=0O  good\n",
      "...                               ...   ...\n",
      "1495            bp0.blogger.com/2015/  good\n",
      "1496            bp0.blogger.com/2011/  good\n",
      "1497         xda-developers.com/2015/  good\n",
      "1498            xda-developers.com/~j  good\n",
      "1499     xda-developers.com/developer  good\n",
      "\n",
      "[1474 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "urls = []\n",
    "\n",
    "urls_data = pd.read_csv(\"data/url_data_scraping.csv\")\n",
    "url_str_list = urls_data[\"url\"]\n",
    "for column in url_str_list:\n",
    "    urls.append(str(column))\n",
    "\n",
    "# create lists to store the urls and the labels\n",
    "url_list = []\n",
    "label_list = []\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=10, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        url_list.append(tokenizer.decode(sample_output, skip_special_tokens=True))\n",
    "        label_list.append('good')\n",
    "        \n",
    "# create data frame of urls\n",
    "url_data_frame = pd.DataFrame(url_list, columns=['url'])\n",
    "# create data frame of labels\n",
    "label_data_frame = pd.DataFrame(label_list, columns=['label'])\n",
    "\n",
    "# create new data frame of both urls and labels\n",
    "X = [url_data_frame[\"url\"], label_data_frame[\"label\"]]\n",
    "headers = [\"url\", \"label\"]\n",
    "X = pd.concat(X, axis=1, keys=headers)\n",
    "X.drop_duplicates(subset=[\"url\", \"label\"], keep=False, inplace=True)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# write the new data set to csv file\n",
    "to_csv = X.to_csv(\"data/transformers_data.csv\", index=False)        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create new generated urls based on DGA urls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  url label\n",
      "0                   weba3e13b0e15094bdfb1.com/go/main   bad\n",
      "1                    weba3e13b0e15094bdfb1.com/go/get   bad\n",
      "2               weba3e13b0e15094bdfb1.com/content/dam   bad\n",
      "3        downloadbed04b373caed221f5.co.uk/item/282801   bad\n",
      "4         downloadbed04b373caed221f5.co.uk/item/v2537   bad\n",
      "5    downloadbed04b373caed221f5.co.uk/showthread.php?   bad\n",
      "6            internetc85b5187e7b1572ce4.org/lib/openv   bad\n",
      "7              internetc85b5187e7b1572ce4.org/lib/x86   bad\n",
      "8       internetc85b5187e7b1572ce4.org/default.aspx (   bad\n",
      "9            linuxda81dea9231d975291.fr/lib/openvda-7   bad\n",
      "10             linuxda81dea9231d975291.fr/lib/x86_64-   bad\n",
      "11   linuxda81dea9231d975291.fr/build/src/sys/modules   bad\n",
      "12        serverf187b7803eb8021406.com/file.php?id=11   bad\n",
      "13       serverf187b7803eb8021406.com/file.php\\n\\n\"\\n   bad\n",
      "14  serverf187b7803eb8021406.com/content/dam/tnc/n...   bad\n",
      "17        winziphfdd7727ea03806fcb.org/pub/software/z   bad\n",
      "18                     webi71ae8a35ce343c91e.fr/t/j4/   bad\n",
      "19                     webi71ae8a35ce343c91e.fr/s/KwY   bad\n",
      "20          webi71ae8a35ce343c91e.fr/viewArticle/1637   bad\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "urls = [\n",
    "    'weba3e13b0e15094bdfb1.com/',\n",
    "    'downloadbed04b373caed221f5.co.uk/',\n",
    "    'internetc85b5187e7b1572ce4.org/',\n",
    "    'linuxda81dea9231d975291.fr/',\n",
    "    'serverf187b7803eb8021406.com/',\n",
    "    'winziphfdd7727ea03806fcb.org/',\n",
    "    'webi71ae8a35ce343c91e.fr/'\n",
    "    ]\n",
    "\n",
    "url_list = []\n",
    "label_list = []\n",
    "\n",
    "for url in urls:\n",
    "    \n",
    "    input_ids = tokenizer.encode(url, return_tensors='tf')\n",
    "\n",
    "    tf.random.set_seed(0)\n",
    "\n",
    "    # set top_k = 50 and set top_p = 0.95 and num_return_sequences = 3\n",
    "    sample_outputs = model.generate(\n",
    "        input_ids,\n",
    "        do_sample=True, \n",
    "        max_length=20, \n",
    "        top_k=50, \n",
    "        top_p=0.95, \n",
    "        num_return_sequences=3\n",
    "    )\n",
    "\n",
    "    for i, sample_output in enumerate(sample_outputs):\n",
    "        url_list.append(tokenizer.decode(sample_output, skip_special_tokens=True))\n",
    "        label_list.append('bad')\n",
    "        \n",
    "# create data frame of urls\n",
    "url_data_frame = pd.DataFrame(url_list, columns=['url'])\n",
    "# create data frame of labels\n",
    "label_data_frame = pd.DataFrame(label_list, columns=['label'])\n",
    "\n",
    "# create new data frame of both urls and labels\n",
    "X = [url_data_frame[\"url\"], label_data_frame[\"label\"]]\n",
    "headers = [\"url\", \"label\"]\n",
    "X = pd.concat(X, axis=1, keys=headers)\n",
    "X.drop_duplicates(subset=[\"url\", \"label\"], keep=False, inplace=True)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# write the new data set to csv file\n",
    "to_csv = X.to_csv(\"data/transformers_dga_data.csv\", index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMe/9c+ALqubr7P1pnuHTxB",
   "collapsed_sections": [],
   "name": "transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
